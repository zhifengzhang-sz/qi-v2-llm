services:
  # Base image that other services depend on
  base:
    build:
      context: ./base
      dockerfile: Dockerfile
      args:
        USERNAME: "${LOCAL_USERNAME:-vscode}"
        USER_UID: "${LOCAL_USER_UID:-1000}"
        USER_GID: "${LOCAL_USER_GID:-1000}"
    image: qi-v2-llm-base:latest
    # Base is just for building, not for direct use
    profiles: ["build-only"]

  typescript:
    build:
      context: . 
      dockerfile: ./typescript/Dockerfile
      args:
        USERNAME: "${LOCAL_USERNAME:-vscode}"
        USER_UID: "${LOCAL_USER_UID:-1000}"
        USER_GID: "${LOCAL_USER_GID:-1000}"
    image: qi-v2-llm-typescript:latest
    volumes:
      - ../typescript-workspace:/workspace:cached
      - ../docs:/workspace/docs:cached
    working_dir: /workspace
    command: sleep infinity

  python:
    build: 
      context: . 
      dockerfile: ./python/Dockerfile
      args:
        USERNAME: "${LOCAL_USERNAME:-vscode}"
        USER_UID: "${LOCAL_USER_UID:-1000}"
        USER_GID: "${LOCAL_USER_GID:-1000}"
    image: qi-v2-llm-python:latest
    volumes:
      - ../python-workspace:/workspace:cached
      - ../docs:/workspace/docs:cached
    working_dir: /workspace
    network_mode: "host"
    command: sleep infinity

  texlive:
    build:
      context: . 
      dockerfile: ./texlive/Dockerfile
      args:
        USERNAME: "${LOCAL_USERNAME:-vscode}"
        USER_UID: "${LOCAL_USER_UID:-1000}"
        USER_GID: "${LOCAL_USER_GID:-1000}"
    image: qi-v2-llm-texlive:latest
    environment:
      - USER=${LOCAL_USERNAME:-vscode}
    volumes:
      - ../texlive-workspace:/workspace:cached
      - ../docs:/workspace/docs:cached
    working_dir: /workspace
    command: sleep infinity
    
  # Enhanced MCP (Model Context Protocol) environment with time series research and cursor integration
  mcp:
    build:
      context: . 
      dockerfile: ./mcp/Dockerfile
      args:
        USERNAME: "${LOCAL_USERNAME:-vscode}"
        USER_UID: "${LOCAL_USER_UID:-1000}"
        USER_GID: "${LOCAL_USER_GID:-1000}"
    image: qi-v2-llm-mcp:latest
    volumes:
      - ../mcp-workspace:/workspace/mcp:cached
      - ../docs:/workspace/mcp/docs:cached
      # Optional shared volume for datasets (uncomment if needed)
      # - ../shared-data:/workspace/mcp/data/shared:cached
    ports:
      - "8000:8000"  # FastAPI server
      - "8888:8888"  # Jupyter Notebook
      - "3000:3000"  # TypeScript client app
    environment:
      - TZ=UTC
      - PYTHONUNBUFFERED=1
      - PYTHONIOENCODING=UTF-8
      - NODE_ENV=development
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}  # For cursor integration
    working_dir: /workspace/mcp
    # Using bridge network instead of host for better container isolation and port mapping
    # network_mode: "host"
    networks:
      - mcp-network
    command: sleep infinity
    # Optional resource limits (uncomment if needed)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '4'
    #       memory: 8G

networks:
  mcp-network:
    driver: bridge